Notes:
======

Ignore any external links
Add a recursion depth max
Listen to robots.txt

Weight the results
Factors:
  - search term in URL
  - search term in text
  - results from POST search


An issue seems to be the sheer number of links on a given page. Eg Camden has 
10000+ links on their website, it's taken hours to crawl (with spidr gem
implementation). Not sure if it's possible to speed that up. Could run this as
delayed scheduled job?

Resources:
==========

Wombat gem:
https://github.com/felipecsl/wombat

Spidr gem:
https://github.com/postmodern/spidr

Mechanize gem (for posting data):
https://github.com/sparklemotion/mechanize
